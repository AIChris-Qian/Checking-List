# 机器学习
  by Chen Qian

------------------------------


**1. `[ 特征工程 ]`**    
------------------------------

#### 01 特征归一化

  Q：为什么需要对数值类型的特征做归一化？
     
     线性函数归一化：   X_norm = (X - X_min) / (X_max - X_min)
     零均值归一化：    Z = (X - μ) / σ
     学习率相同的情况下，更新速度不同。

#### 02 类型型特征

   Q：在对数据进行预处理时，应该怎么样处理类别型特征？
    
     (1) 序号编码：高 / 中 / 低   3 / 2 / 1
     (2) 独热编码：血型 (A,B,AB,O) A：(1,0,0,0) B：(0,1,0,0) AB：(0,0,1,0) O：(0,0,0,1) 
     (3) 二进制编码：每个类别赋予一个ID，然后转化为二进制 (维度少于独热编码，节省空间)
   

#### 03 高维组合特征的处理

  Q：什么是组合特征？如何处理高维组合特征？
      
     高阶组合特征：为了提高拟合能力，把一阶离散特征两两组合。
     组合特征 <Xi,Xj> 维度 m * n 太大，使用 k 维的低维向量(k<<m,k<<n) 
     参数规模：m * k + n * k  (矩阵分解)

#### 04 组合特征

   Q：怎样有效地找到组合特征？
       
     决策树： 根据原始输入特征和标签构造决策树。
     GBDT：  在之前构建的决策树残差上构造下一棵决策树。


------------------------------


**2. `[ 模型评估 ]`**    
------------------------------

                  预测为正       预测为反
     实际为正    TP (真正例)    FN (假反例)
     实际为反    FP (假正例)    TN (真反例)


#### 01 评估指标的局限性

   Q：准确率的局限性
      
     准确率 (Accuracy) = (TP + TN) / (TP + FN + FP + TN)
      
     正负样本比 = 1 : 99     把所有样本预测为负样本准确率 99%
     不同类别的样本比例不均衡，占比大的类别会影响准确率。
      

   Q：精确率与召回率的权衡
      
     精确率 (Precision) = TP / (TP + FP)                            排序模型：TOP N 返回的结果被判定为正样本，计算前 N 位置上的精确率和召回率
     召回率 (Recall) = TP / (TP + FN)                               
      
     绘制 P - R 曲线，横轴是召回率，纵轴是精确率。                     F1 score：精确率和召回率的调和平均数    1/F1 = 1/2 * (1/P + 1/R) 
     确定阈值，大于该阈值的判定为正样本，小于该阈值的判定为负样本。
     通过把阈值从高到低移动生成 P - R 曲线。原点附近阈值最大。
      
   
   Q：平均根误差的意外 
      
     RMSE (均方根误差) = √￣(∑(yi - yi~)^2/n)     MSE (均方误差) = ∑(yi - yi~)^2/n        yi：真实值    yi~：预测值    n：样本个数
      
     95% 的区间误差小于 1%，RMSE很差，可能由于其余 5% 区间存在严重的离散点
     解决方法      (1) 如果离群点是噪声点，数据预处理把噪声过滤
                  (2) 如果离群点不是噪声点，进一步提升模型预测能力
                  (3) 找更合适的指标来评估模型：  MAPE (平均绝对百分比误差) = ∑|(yi - yi~) / yi| * 100/n
                                                把每个点的误差进行归一化，降低离群点带来的影响
   

#### 02 ROC曲线

   Q：什么是ROC曲线？
      
     受试者工作特征曲线，横坐标为假阳性率，纵坐标为真阳性率
      
     真阳性率 (TPR) = TP / (TP + FN)                           
     假阳性率 (FPR) = FP / (FP + TN)      

   Q：如何绘制ROC曲线？
      
     不断移动分类器的截断点来生成曲线上的一些关键点。
     指定阈值，大于该阈值判为正例，小于该阈值判为负例。截断点就是阈值。
     从最高得分开始 (ROC曲线零点)，逐渐调整到最低分。
      
     简便方法：从零点开始，每遇到一个正样本就沿纵轴方向，每遇到一个负样本就沿横轴方向。
      

   Q：如何计算AUC？
      
     ROC曲线下面积大小，量化ROC曲线衡量出的模型性能。
     计算AUC，沿着ROC横轴做积分。AUC取值在 0.5 ~ 1 之间。
     AUC越大，分类性能越好。


   Q：ROC曲线相比 P - R 曲线有什么特点？
      
     当正负样本的分布发生变化时，ROC曲线的形状能保持基本不变，P - R 曲线会发生剧烈的变化。
     ROC曲线能降低不同测试集带来的干扰，更客观地衡量模型的性能。
     实际问题中，正负样本数量往往不均衡。ROC使用场景：排序 / 推荐 / 广告。
      

#### 03 余弦距离的应用
   
     余弦相似度：   两个特征向量之间的相似性。取值范围为 [-1,1]，相同的两个向量之间相似度为1。
     余弦距离：    1 - 余弦相似度，取值范围为 [0,2]，相同的两个向量余弦距离为0。
    

   Q：为什么在一些场景要使用余弦相似度而不是欧氏距离？
     
     余弦相似度： cos(A,B) = (A * B) / (|A| * |B|)
      
     余弦相似度在高维下，保持" 相同时为1，正交时为0，相反时为-1"。
     欧氏距离在高维情况下范围不确定。
      
     模长经过归一化，欧氏距离与余弦距离的关系：|A - B| = √￣(2(1 - cos(A,B)))
      
     欧式距离：数值上的绝对差异    (用户活跃度：登录次数 / 平均观看时长)
     余弦距离：方向上的相对差异    (用户偏好)

   
   
   Q：余弦距离是否一个严格定义的距离？
     
     不是
     
     距离：每一对元素确定一个实数，使得 (正定性 / 对称性 / 三角不等式) 成立，这个实数称为距离。
      
     余弦距离：dist(A,B) = 1 - cos(A,B)
          (1)  正定性：cos(A,B) ≤ 1，dist(A,B) ≥ 0  成立
          (2)  对称性：dist(A,B) = 1 - cos(A,B) = 1 - cos(B,A) = dist(B,A)  成立
          (3)  三角不等式：A = (1,0) B = (1,1) C = (0,1) 
                          dist(A,B) + dist(B,C) < dist(A,C)  不成立
      
   

   
#### 04 AB测试的陷阱

   Q：对模型进行充分的离线评估后，为什么还要进行在线AB测试？


     (1) 离线评估无法完全消除过拟合的影响
     (2) 离线评估是在理想环境下，没有考虑数据丢失的情况
     (3) 某些商业指标在离线评估中无法计算
          离线评估关注：  ROC曲线 / P — R 曲线
          线上评估关注：  用户点击率 / 留存时长 / PV访问量


   Q：如何进行线上AB测试？
        
     用户分成实验组和对照组   实验组：新模型    对照组：旧模型


   Q：如何划分实验组和对照组？
        
     基于设备号 / 用户ID尾号进行分组 (奇数分为实验组，偶数分为对照组)

#### 05 模型评估的方法


   Q：在模型评估过程中，有哪些主要的验证方法？优缺点是什么？
        
     (1)  Holdout 检验：70% 训练集，模型训练。30% 验证集，模型验证。
                       缺点：验证集上计算出来的评估指标与原始分组有关
                       
     (2)  交叉检验：
          k-fold 交叉检验：样本分成 k 个大小相等的样本子集，依次遍历这 k 个子集。
                          当前子集作为验证集，其余子集作为训练集。最后把 k 次评估指标平均值作为最终的评估指标。
          留 p 验证：样本总数为 n，依次对 n 个样本进行遍历。每次留下 p 个样本作为验证集，其余样本作为测试集。 C(n,p)
                    时间花费远大于留一验证。
     
     (3)  自助法：
          样本数为 n，进行 n 次有放回的随机抽样，得到大小为 n 的训练集。n 次采样中，没有被抽出的样本作为验证集。(适合小样本)
          
          
                    
   Q：在自助法的采样过程中，对 n 个样本进行 n 次自助抽样，当 `n → ∞`，有多少数据从未被选过？
        
     (1 - 1/n)^n  lim(n → ∞)(1 - 1/n)^n = 1/e ≈ 0.368 
       
       
#### 06 超参数调优

   Q：数据集分为哪几类？
     
     数据集：训练集 / 验证集 / 测试集
            训练集：总样本的 70% - 80%，用于模型训练。
            验证集：不参与模型训练，对超参数进行选择。
            测试集：结果未知，利用训练模型输出。
            
     模型上线后，输出测试集上的结果，与实际结果进行对比。
     测试集后续转化为训练集 / 验证集，实现模型的不断迭代与优化。


   Q：简述参数与超参数之间的区别？

     参数：通过模型对训练集的拟合获得的。
     超参数：在训练前人为给出超参数。通过验证集验证。
     
     调参：调整超参数
   

   Q：超参数有哪些调优方法？

     网格搜索 / 随机搜索 / 贝叶斯优化算法

#### 07 过拟合与欠拟合


   Q：在模型评估中，过拟合和欠拟合具体指什么现象？

     过拟合：模型在训练集上表现很好，在测试集上表现很差。
            模型过于复杂，把噪声数据特征也学习了，导致模型泛化能力下降。
            
     欠拟合：模型在训练集和测试集上表现都不好的情况。
            模型过于简单不能很好地拟合数据。



   Q：能否说说几种降低过拟合和欠拟合风险的方法？

     (1) 降低过拟合的方法
         更多训练数据，学习更多特征，减少噪声的影响。
         降低模型复杂度，避免模型拟合过多的噪声。
         正则化方法。加入损失函数，避免过拟合风险。
         集成学习方法。多个模型集成在一起，降低单一模型的过拟合风险。

     (2) 降低欠拟合的方法
         增加新特征，特征不足时容易出现欠拟合。
         增加模型复杂度，增加复杂度提升模型的拟合能力。



------------------------------


**3. `[ 经典算法 ]`**    
------------------------------

#### 01 支持向量机

   Q：支持向量机原理？
     
     线性可分：存在一个超平面可以将训练样本正确分类
     
     找到一个最大软间隔的超平面，使得训练样本可以正确分类。
     如果原始空间不存在这样一个超平面，可以使用核函数把样本从原始空间映射到一个更高维的空间，使得样本在这个空间里线性可分。
     
   Q：常见核函数？
     
     线性核 / 多项核 / 高斯核 / 拉普拉斯核 / Sigmoid核


#### 02 逻辑回归

   Q：逻辑回归推导？
     
     似然估计相乘，然后取对数
     
   Q：逻辑回归相比线性回归，有何异同？
     
     逻辑回归：分类问题，因变量为离散，广义线性模型因变量 y 服从二元分布。
     线性回归：预测问题，因变量为连续，最小二乘法求解因变量 y 服从正态分布。
     
     相同：两者都使用极大近似估计来对训练样本建模。求解超参数时都使用梯度下降。


   Q：当使用逻辑回归处理多标签的分类问题时，有哪些常用方法？
     

     (1) 一个样本对应一个标签，概率服从几何分布，使用 softmax 进行分类。
     
     (2) 一个样本对应多个标签，训练 k 个二分类的逻辑回归分类器。
         第 i 个分类器：标签是否归为第 i 类。

#### 03 决策树
     
     决策树 (集成学习) → 随机森林 / 梯度提升决策树
     决策树的生成：特征选择 / 树的结构 / 树的剪枝

   Q：决策树有哪些常用的启发函数？
     
     ID3 / C4.5 / CART
     
     (1) ID3 - 最大信息增益
     (2) C4.5 - 最大信息增益比
     (3) CART - 最小基尼指数
     
     ID3：会倾向于选择取值较多的特征，特征越多，信息增益越大。
     C4.5：ID3优化，对取值较多的特征进行惩罚，避免ID3过拟合。
     
     离散型变量：ID3           连续性变量：C4.5 / CART
     分类任务：ID3 / C4.5      分类 / 回归任务：CART
     
     
 
   Q：如何对决策树进行剪枝？
     
     完全生长的决策树 → 过拟合   通过剪枝，提升模型的泛化能力。
     剪枝：预剪枝 / 后剪枝
     
     (1) 预剪枝：生成决策树的过程中提前停止树的增长   (容易欠拟合)
     (2) 后剪枝：已生成的过拟合决策树进行剪枝         (有更强的泛化能力，时间更长)
     
         
     
         
------------------------------


**4. `[ 降维 ]`**    
------------------------------
     
     高维空间包含很多噪声，用降维方式寻找数据内部的特性。
     
#### 01 PCA最大方差理论

         
   Q：定义主成分？如何设计目标函数？如何针对 PCA 问题进行求解？
   
     主轴上数据分布越分散，数据在这个方向方差越大。
     PCA目标：最大化投影方差，数据在主轴上的投影的方差最大。
     
     PCA求解方法
     (1) 样本数据中心化处理                                        {x1,x2,.....} = {v1 - μ,v2 - μ,......}
     (2) 样本协方差矩阵                                            1/n * ∑(xi * xi^T)
     (3) 特征值求解，特征值从大到小排列                             {λ1,λ2,λ3,......}
     (4) 特征值前 d 大对应的特征向量 w1,W2,...., n 维映射到 d 维     xi~ = [w1^T * xi,w2^T * xi,.....,wd^T * xi]^T 
     

 
------------------------------


**5. `[ 非监督学习 ]`**    
------------------------------

#### 01 k均值聚类

   Q：阐述监督学习和非监督学习的区别？
      
     监督学习：训练集既有特征，又有标签。测试集通过训练获得标签。
              标签连续：预测      线性回归 / 神经网络 / 时间序列
              标签离散：分类      逻辑回归 / 支持向量机 / 决策树
              
     非监督学习：训练集只有特征，没有标签。
                聚类：把样本分成若干类      k均值 / EM算法
                降维：高维度数据           主成分分析
                

   Q：参数模型和非参数模型的区别和优缺点？
      
     参数模型：训练前确定目标函数          逻辑回归 / 线性回归 / 朴素贝叶斯
     非参数模型：训练前没有确定目标函数    支持向量机 / 决策树
     
     参数优点：模型训练快，数据量小
         缺点：需要提前对目标函数做假设。复杂度低，容易欠拟合
              
     非参数优点：无需做假设，训练模型能逼近任意复杂的真实模型
           缺点：模型复杂计算量大。
           
   
   Q：简述k均值算法的具体步骤？
      
     通过迭代方法寻找 k 个簇的一种划分方案，使得聚类结果的代价函数最小。
     
     (1) 数据预处理：归一化 / 离群点处理
     (2) 随机选取 k 个簇中心
     (3) 定义代价函数
     (4) 重复下面过程直到代价函数收敛
                                    对于每一个样本，将其分配到距离最近的簇
                                    对于每一个类簇，重新计算该类簇的中心
                                    
     k均值算法迭代，代价函数没有收敛时，固定簇中心，更新每个样本所属的簇类别。然后固定簇类别，更新簇中心。直到代价函数收敛。


   
   Q：k均值算法的优缺点是什么？如何对其调优？
      
     缺点：通常是局部最优解 / 无法解决数据簇分布差异大的情况
     优点：k均值算法是高效的，复杂度时线性的。
     
     调优：
     (1) 数据归一化和离群值处理
     (2) 合理选择 k 值：手肘法 / Gap Statistic
     (3) 采取核函数：通过一个非线性映射，将输入空间中的数据点映射到高维的特征空间中，在新的特征空间中进行聚类。



------------------------------


**6. `[ 概率图模型 ]`**    
------------------------------

#### 01 概率图表示

   Q：解释朴素贝叶斯模型的原理，并给出概率图模型？
      
     预测样本所属类别     y = max(yi|x)   p(yi|x) = p(x|yi) * p(yi) / p(x) 
                        p(x) 作为先验概率，对于特定样本 x，p(x) 取值相同。
                        p(x) ∝  p(x|yi) * p(yi) = p(x1|yi) * p(x2|yi) ..... p(xn|yi) * p(yi)   假设x1,x2....xn相互独立 


#### 02 生成式模型与判别式模型


   Q：哪些是生成式模型，哪些是判别式模型？
      
     生成式模型：联合概率分布 p(x,y,z)  观测变量 x，预测变量 y，其他变量 z 共同出现的概率
     判别式模型：条件概率分布 p(y,z|x)  观测变量 x的情况下，预测变量 y 和其他变量 z 出现的概率
     
     生成式：朴素贝叶斯
     判别式：逻辑回归 / 支持向量机 / 决策树



------------------------------


**7. `[ 优化算法 ]`**    
------------------------------


     机器学习算法 = 模型表征 + 模型评估 + 优化算法
     支持向量机：线性分类模型 + 最大间隔
     逻辑回归：线性分类模型 + 交叉熵
     
#### 01 有监督学习的损失函数


   Q：监督学习涉及的损失函数有哪些？
      
     Xi：样本点   Yi：标签   θ：参数   f(,θ)：X → Y   L：损失函数
     L(f(xi,θ),yi)越小，样本匹配越好。
     
     (1) 二分类问题      0-1损失 / Hinge损失 / Logistic损失 / 交叉熵
     (2) 回归问题        平方损失 / 绝对损失 / Huber损失
                   


#### 02 机器学习中的优化问题


   Q：哪些是凸优化问题，哪些是非凸优化问题？
      
     凸优化：    二分类问题 / 支持向量机 / 线性回归
     非凸优化：  主成分分析 / 奇异值分解 / 矩阵分解 / 深度神经网络


#### 03 经典优化算法



   Q：无约束优化问题的优化方法有哪些？
      
     (1) 直接法：L是凸函数，L有闭式解
     (2) 迭代法：一阶法 (梯度下降法)   二阶法 (牛顿法)


#### 04 随机梯度下降法



   Q：训练数据量特别大时，经典的梯度下降存在什么问题？怎么改进？
      
     问题：需要遍历所有的训练数据，计算大，费时间。
     
     随机梯度下降：单个训练样本的损失来近似平均损失，加快收敛。
     小批量梯度下降：同时处理若干数据，降低随机梯度的方差。


#### 05 L1正则化与稀疏性


   Q：L1正则化使得模型参数具有稀疏性的原理是什么？
     
     引入惩罚项，使得各变量的系数得以收缩，避免过拟合。
     L1：惩罚系数的绝对值    L2：惩罚系数的平方
      
     (1) 解空间的形状：L1的解空间是多边形，L2的解空间是圆形。
                      多边形的解空间更容易在尖角处与等高线碰撞出稀疏解。
     
     (2) 函数叠加：目标函数 L(w)，L2正则化 L(w)+Cw^2，L1正则化 L(w)+C|w|
                  L1最小值在原点，产生稀疏性。L2只有减小 w 绝对值作用。
                  
     (3) 贝叶斯检验：L1正则化引入拉普拉斯先验，L2正则化引入高斯先验。
                    拉普拉斯使参数为0的可能性更大。




------------------------------



**8. `[ 前向神经网络 ]`**    
------------------------------

     前馈：从输入到输出过程中不存在模型自身的反馈连接。


#### 01 深度神经网络中的激活函数



   Q：常用的激活函数及其导数？
      
     (1) Sigmoid 函数    取值范围 (0,1)
     (2) Tanh 函数       取值范围 (-1,1)
     (3) ReLU            取值范围 [0,+∞)



   Q：为什么 Sigmoid 和 Tanh 激活函数会导致梯度消失的现象？
      
     Sigmoid 和 Tanh 函数的导数在很大和很小时都会趋近于0，梯度消失。
     
     
   Q：ReLU 的激活函数相对于 Sigmoid 和 Tanh 激活函数的优点？
      
     优点：(1) Sigmoid 和 Tanh 复杂度高
           (2）ReLU 可以有效解决梯度消失的问题
     
     缺点：负梯度在经过该 ReLU 单元时被放置0，之后不被任何函数激活，永远是0。
           Leakay ReLU   f(z) = {z,z>0  az,z≤0}  既实现了单侧抑制，又保留了负梯度信息。



#### 02 多层感知机的反向传播算法


     反向传播：将损失函数的信息沿网络层向后传播以计算梯度，达到优化效果。


   Q：多层感知机的平方误差和交叉熵损失函数？
      
     代价函数 = 平方误差项 + L2正则化
     代价函数 = 交叉熵 + L2正则化
     


   Q：平方误差损失函数和交叉熵损失函数分别适用什么场景？
      
     平方损失函数：输出为连续，最后一层不含 Sigmoid 或 Softmax 激活函数
     交叉熵损失函数：输出为离散，二分类 / 多分类场景。


#### 03 神经网络训练技巧


     解决过拟合：数据增强 / 正则化 / 模型集成
     神经网络训练：学习率 / 权重衰减系数 / Dropout比例

   Q：神经网络训练时是否可以将全部参数初始化为0？
      
     不行，最终同一层网络各个参数都是相同的。
     初始化参数可以考虑均匀分布。
       
  
   Q：为什么Dropout可以抑制过拟合？工作原理？
      
     Dropout：某个神经元节点激活值以一定概率 p 丢弃。
     
     工作原理：训练阶段，每个神经元节点增加一个概率系数 (前向传播 + 反向传播)
              测试阶段 (前向传播)


   Q：批量归一化的基本动机与原理是什么？
      
     训练和测试数据分布不同         x~(k) = (x(k) - E[x(k)]) / √￣(Var[x(k)])    y(k) = γ(k) * x~(k) + β(k)
     
       


------------------------------



**9. `[ 集成学习 ]`**    
------------------------------

     多个分类器的结果统一成一个最终的决策
     基分类器：每个单独的分类器


#### 01 集成学习的种类

   Q：集成学习分哪几种？他们有何异同？
     
     Boosting：各个基分类器间有依赖。基分类器层层叠加，每一层训练时，对前一层基分类器分错的样本给予更高的权重。各基分类器结果加权得到最终的结果。
     
     Bagging：各个基分类器间无依赖。可以并行训练。每个个体单独做出判断，通过投票方式做出集体决策。
     
     偏差：训练误差不收敛   方差：容易产生过拟合    错误：偏差 + 方差
     基分类器：弱分类器，错误率要大于集成分类器。
     
     Boosting：减小集成分类器的偏差  Bagging：分而治之，减小集成分类器的方差

#### 02 集成学习的步骤和例子

   Q：集成学习有哪些基本步骤？举几个集成学习的例子？
   
     (1) 找到误差相对独立的基分类器
     (2) 训练基分类器
     (3) 合并基分类器的结果 (voting / stacking)
     
     voting：投票，获得最多选票的结果作为最终的结果。
     stacking：串行，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加。
     
     Adaboost (1) 确定基分类器：ID3决策树，任何分类模型都可以作为基分类器
              (2) 训练基分类器：        循环
                               (从训练集中采样出子集 St
                               用 st 训练基分类器 ht
                               计算 ht 的错误率
                               计算基分类器 ht 权重 at
                               设置下一次采样)
                               
              (3) 合并基分类器：给定一个未知样本 z，输出分类结果为 sign(∑ht(z)at)
     
     思想：分类错误的样本提升权重，分类正确的样本降低权重。
     
     GBDT：每一颗树学的是之前所有树结论和的残差
     
                                                              
     
#### 03 基分类器

   Q：常用的基分类器是什么？
   
   Q：可否将随机森林中的基分类器，由决策树替换成线性分类器或 k-近邻？

#### 04 偏差和方差

   Q：什么是偏差和方差？
   
   Q：如何从减小方差和偏差角度解释 Boosting 和 Bagging 的原理？

#### 05 梯度提升决策树


   Q：GBDT的基本原理是什么？
   
   Q：梯度提升和梯度下降的区别和联系是什么？
   
   Q：GBDT的优点和局限性有哪些？   
   

#### 06 XGBoost 与 GBDT 的联系与区别


   Q：XGBoost 与 GBDT 的联系和区别有哪些？
